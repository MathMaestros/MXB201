%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\usepackage{mathdots}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\nullity}{nullity}

\usepackage{changepage} % Modify page width
\usepackage{multicol} % Use multiple columns
\usepackage[explicit]{titlesec} % Modify section heading styles

\titleformat{\section}{\raggedright\normalfont\bfseries}{}{0em}{#1}
\titleformat{\subsection}{\raggedright\normalfont\small\bfseries}{}{0em}{#1}

%% A4 page
\geometry{
a4paper,
margin = 10mm
}

%% Hide horizontal rule 
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}

%% Hide page numbers
\pagenumbering{gobble}

%% Multi-columns setup
\setlength\columnsep{4pt}

%% Paragraph setup
\setlength\parindent{0pt}
\setlength\parskip{0pt}

%% Customise section heading styles
% \titleformat*\section{\raggedright\bfseries}

\begin{document}
% Modify spacing
\titlespacing*\section{0pt}{1ex}{1ex}
\titlespacing*\subsection{0pt}{1ex}{1ex}
%
\setlength\abovecaptionskip{8pt}
\setlength\belowcaptionskip{-15pt}
\setlength\textfloatsep{0pt}
%
\setlength\abovedisplayskip{1pt}
\setlength\belowdisplayskip{1pt}

\begin{multicols*}{3}
    \subsection{General Solution to a Linear System}
    If \(\symbfit{b} \in \columnspace{A}\): \(\symbfit{x}_g = \symbfit{x}_p + \symbfit{x}_n\)
    where \(\symbfit{x}_p \in \R^n\) and \(\symbfit{x}_n \in \nullspace{A}\).
    \subsection{Minimum Norm Solution}
    \(\symbfit{x}_r \in \rowspace{A}\) where \(\symbfit{x}_r = \proj_{\rowspace{A}}{\left( \symbfit{x}_g \right)}\).
    \section{Least Squares (LS)}
    If \(\symbfit{b} \not\in \columnspace{A}\): \(\symbfit{x} = \argmin_{\symbfit{x}^\ast \in \R^n} \norm{\symbfit{b} - \symbf{A}\symbfit{x}^\ast}\).
    \(\symbfit{b} - \symbf{A} \symbfit{x} \in \nullspace{A} \implies \symbf{A}^\top \left( \symbfit{b} - \symbf{A}\symbfit{x} \right) = \symbfup{0}\).
    \begin{align*}
        \symbf{A}^\top \symbf{A}\symbfit{x} & = \symbf{A}^\top \symbfit{b} &  & \text{(Normal Equations)}
    \end{align*}
    \subsection{Orthogonal Projection}
    \begin{align*}
        \symbf{P}             & = \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top     \\
        \symbf{P} \symbfit{b} & = \proj_{\columnspace{A}} \left( \symbfit{b} \right) = \symbf{A}\symbfit{x}
    \end{align*}
    \(\symbf{P}\) is idempotent (\(\symbf{P}^2 = \symbf{P}\)) and \(\symbf{P}^\top = \symbf{P}\).
    \subsection{Dependent Columns}
    If \(\nullity{\left( \symbf{A} \right)} > 0\), NE yields infinitely many solutions
    as \(\nullspace{A} = \mathcal{N}\left(\symbf{A}^\top \symbf{A}\right)\).
    \subsection{Orthogonal Complement Projections}
    Given \(\symbf{P} = \proj_V\): \(\symbf{Q} = \proj_{V^\perp} = \symbf{I} - \symbf{P}\)
    \begin{equation*}
        \symbfit{b} = \proj_V(\symbfit{b}) + \proj_{V^\perp}(\symbfit{b}) = \symbf{P}\symbfit{b} + \symbf{Q}\symbfit{b}
    \end{equation*}
    \begin{align*}
        \left( \symbf{P}\symbfit{b} \right)^\top \symbf{Q}\symbfit{b} & = 0                                   \\
        \symbf{P} \symbf{Q}                                           & = \symbf{0} &  & \text{(zero matrix)}
    \end{align*}
    \subsection{Change of Basis}
    Given the basis \(W = \left\{ \symbfit{w}_1,\: \dots,\: \symbfit{w}_n \right\}\)
    \begin{align*}
        \symbfit{b} & = c_1 \symbfit{w}_1 + \cdots + c_n \symbfit{w}_n                         \\
        \symbfit{b} & = \symbf{W} \symbfit{c} \iff \left( \symbfit{b} \right)_W = \symbfit{c}.
    \end{align*}
    \subsection{Orthonormal Basis}
    Normalised and orthogonal basis vectors.
    For \(Q = \left\{ \symbfit{q}_1,\: \dots,\: \symbfit{q}_n \right\}\),
    \(\symbfit{q}_i^\top \symbfit{q}_j = \delta_{ij}\), where
    \begin{equation*}
        \delta_{ij} = \begin{cases}
            1, & i = j   \\
            0, & i \ne j
        \end{cases}
    \end{equation*}
    \begin{equation*}
        \symbf{Q} \symbfit{c} = \symbfit{b}
        \iff
        \symbf{Q}^\top \symbfit{b} = \symbfit{c} = \left( \symbfit{b} \right)_Q
    \end{equation*}
    \subsection{Orthogonal Matrices}
    \begin{equation*}
        \symbf{Q}^\top = \symbf{Q}^{-1}
        \iff
        \symbf{Q}^\top \symbf{Q} = \symbf{Q}\symbf{Q}^\top = \symbf{I}.
    \end{equation*}
    \subsection{Projection onto a Vector}
    \begin{align*}
        \proj_{\symbfit{a}} \left( \symbfit{b} \right) & = \symbfit{a} \left( \symbfit{a}^\top \symbfit{a} \right)^{-1} \symbfit{a}^\top \symbfit{b} \\
                                                       & = \frac{\symbfit{a}}{\norm{\symbfit{a}}^2} \symbfit{a} \cdot \symbfit{b}
    \end{align*}
    Using a unit vector \(\symbfit{q}\):
    \begin{equation*}
        \proj_{\symbfit{q}} \left( \symbfit{b} \right) = \symbfit{q} \left( \symbfit{q} \cdot \symbfit{b} \right)
    \end{equation*}
    \subsection{Gram-Schmidt Process}
    Converts the basis \(W\) that spans \(\columnspace{A}\) to an orthonormal basis \(Q\).
    \begin{align*}
        \symbfit{v}_1 & = \symbfit{w}_1                                                                                       & \symbfit{q}_1 & = \hat{\symbfit{v}}_1 \\
        \symbfit{v}_2 & = \symbfit{w}_2 - \symbfit{q}_1 \left( \symbfit{q}_1 \cdot \symbfit{w}_2 \right)                      & \symbfit{q}_2 & = \hat{\symbfit{v}}_2 \\
                      & \vdotswithin{=}                                                                                       &               & \vdotswithin{=}       \\
        \symbfit{v}_i & = \symbfit{w}_i - \sum_{j = 1}^{i - 1} \symbfit{q}_j \left( \symbfit{q}_j \cdot \symbfit{w}_i \right) & \symbfit{q}_i & = \hat{\symbfit{v}}_i
    \end{align*}
    \(V\) and \(Q\) span \(W\), and \(V\) is orthogonal.
    \subsection{QR Decomposition}
    \begin{equation*}
        \symbf{A} = \symbf{Q} \symbf{R}
    \end{equation*}
    where \(\symbf{Q}\) is orthogonal and \(\symbf{R}\) is upper triangular. \(\symbf{R} \symbfit{x} = \symbf{Q}^\top \symbfit{b}\) solves LS.
    \section{Eigenvalues and Eigenvectors}
    \begin{equation*}
        \symbf{A} \symbfit{v} = \lambda \symbfit{v} \iff \left( \lambda \symbf{I} - \symbf{A} \right) \symbfit{v} = \symbfup{0} : \symbfit{v} \neq \symbfup{0}
    \end{equation*}
    \subsection{Characteristic Polynomial}
    \begin{equation*}
        P\left( \lambda \right) = \det{\left( \lambda \symbf{I} - \symbf{A} \right)} = 0.
    \end{equation*}
    \subsection{Eigen Decomposition}
    \begin{align*}
        \symbf{A} \symbf{V} = \symbf{V} \symbf{D}
                  & \iff
        \symbf{A} = \symbf{V} \symbf{D} \symbf{V}^{-1}                       \\
        \symbf{V} & = \begin{bmatrix}
                          \symbfit{v}_1 & \cdots & \symbfit{v}_n
                      \end{bmatrix}                 \\
        \symbf{D} & = \diag{\left( \lambda_1,\: \dots,\: \lambda_n \right)}.
    \end{align*}
    \subsection{Algebraic Multiplicity \texorpdfstring{\(\mu\left( \lambda_i \right)\)}{mu(lambda i)}}
    Multiplicity of \(\lambda_i\) in \(P(\lambda)\),
    for \(d \leq n\) distinct eigenvalues,
    \begin{equation*}
        P\left( \lambda \right) = \left( \lambda - \lambda_1 \right)^{\mu\left( \lambda_1 \right)} \cdots \left( \lambda - \lambda_d \right)^{\mu\left( \lambda_d \right)}.
    \end{equation*}
    In general
    \begin{gather*}
        1 \leq \mu\left( \lambda_i \right) \leq n         \\
        \sum_{i = 1}^d \mu \left( \lambda_i \right) = n
    \end{gather*}
    If \(\nullity{\left( \symbf{A} \right)} > 0\)
    \begin{equation*}
        \exists k : \lambda_k = 0 : \mu\left( \lambda_k \right) = \nullity{\left( \symbf{A} \right)}
    \end{equation*}
    \subsection{Geometric Multiplicity \texorpdfstring{\(\gamma\left( \lambda_i \right)\)}{gamma(lambda i)}}
    Dimension of the eigenspace associated with \(\lambda_i\).
    \begin{equation*}
        \gamma \left( \lambda_i \right) = \nullity{\left( \lambda_i \symbf{I} - \symbf{A} \right)}.
    \end{equation*}
    Given \(d \leq n\) distinct eigenvalues,
    \begin{gather*}
        1 \leq \gamma\left( \lambda_i \right) \leq \mu\left( \lambda_i \right) \leq n \\
        d \leq \sum_{i = 1}^d \gamma \left( \lambda_i \right) \leq n.
    \end{gather*}
    Eigenvectors corresponding to distinct eigenvalues are linearly dependent.
    \subsection{Defective Matrix}
    \(\symbf{A}\) lacks a complete eigenbasis:
    \begin{equation*}
        \exists k : \gamma\left( \lambda_k \right) < \mu\left( \lambda_k \right)
    \end{equation*}
    \subsection{Matrix Similarity}
    \(\symbf{A}\) and \(\symbf{B}\) are similar if
    \begin{equation*}
        \symbf{B} = \symbf{P}^{-1} \symbf{A} \symbf{P}.
    \end{equation*}
    They share \(P(\lambda)\), ranks, determinants, traces, and eigenvalues (also \(\mu\) and \(\gamma\)).
    \subsection{Symmetric Matrices \texorpdfstring{\(\symbf{S}^\top = \symbf{S}\)}{S' = S}}
    \(\symbf{S}\) is always diagonalisable and has
    real eigenvalues with real orthogonal eigenvectors: \(\symbf{S} = \symbf{Q} \symbf{D} \symbf{Q}^\top\).
    \subsection{Skew-Symmetric Matrices \texorpdfstring{\(\symbf{K}^\top = -\symbf{K}\)}{K' = -K}}
    Eigenvalues are always purely imaginary.
    \subsection{Positive-Definite Matrices}
    \(\symbf{M}\) is (symmetric) positive definite if all its eigenvalues are positive. Also
    \begin{equation*}
        \symbfit{x}^\top \symbf{M} \symbfit{x} > 0 : \forall \symbfit{x} \in \R^n \backslash \left\{ \symbfup{0} \right\}
    \end{equation*}
    \subsection{Matrix Functions}
    Given a nondefective matrix:
    \begin{align*}
        f\left( \symbf{A} \right) & = \symbf{V} f\left( \symbf{D} \right) \symbf{V}^{-1}                                                               \\
                                  & = \symbf{V} \diag{\left( f\left( \lambda_1 \right),\: \ldots,\: f\left( \lambda_n \right) \right)} \symbf{V}^{-1}.
    \end{align*}
    for an analytic function \(f\).
    \subsection{Cayley-Hamilton Theorem}
    \begin{align*}
        \forall \symbf{A} : P\left( \symbf{A} \right) = \symbfup{0} &  & \text{(zero matrix)}
    \end{align*}
    \section{Singular Value Decomposition}
    \begin{gather*}
        \symbf{A} \symbf{V} = \symbf{U} \symbf{\Sigma}
        \iff
        \symbf{A} = \symbf{U} \symbf{\Sigma} \symbf{V}^\top                                        \\
        \symbf{V}^\top = \symbf{V}^{-1}, \quad \symbf{U}^\top = \symbf{U}^{-1}                                                          \\
        \symbf{\Sigma} = \diag{\left( \sigma_1,\: \dots,\: \sigma_r,\: 0,\: \dots,\: 0 \right)}.
    \end{gather*}
    Left singular vectors \(\symbfit{u}\): \(\symbf{U} \in \R^{m \times m}\)
    \begin{align*}
        \columnspace{A}   & = \vspan{\left( \left\{ \symbfit{u}_{i \leq r} \right\} \right)}     \\
        \leftnullspace{A} & = \vspan{\left( \left\{ \symbfit{u}_{r < i \leq m} \right\} \right)}
    \end{align*}
    Right singular vectors \(\symbfit{v}\): \(\symbf{V} \in  \R^{n \times n}\)
    \begin{align*}
        \rowspace{A}  & = \vspan{\left( \left\{ \symbfit{v}_{i \leq r} \right\} \right)}     \\
        \nullspace{A} & = \vspan{\left( \left\{ \symbfit{v}_{r < i \leq n} \right\} \right)}
    \end{align*}
    Singular values \(\sigma_i\): \(\symbf{\Sigma} \in \R^{m \times n}\)

    The eigenvalues of \(\symbf{A}^\top\symbf{A}\) and \(\symbf{A}\symbf{A}^\top\)
    are equal, \(\symbf{\Sigma}^\top \symbf{\Sigma}\) and \(\symbf{\Sigma} \symbf{\Sigma}^\top\) have the same diagonal entries, and
    when \(m = n\), \(\symbf{\Sigma}^\top\symbf{\Sigma} = \symbf{\Sigma} \symbf{\Sigma}^\top = \symbf{\Sigma}^2\).
    To find \(\sigma_i\) compute:
    \begin{align*}
        \symbf{A}^\top \symbf{A} & = \symbf{V} \symbf{\Sigma}^\top \symbf{\Sigma} \symbf{V}^\top \\
        \symbf{A} \symbf{A}^\top & = \symbf{U} \symbf{\Sigma} \symbf{\Sigma}^\top \symbf{U}^\top
    \end{align*}
    so that \(\sigma_i = \sqrt{\lambda}_i\) where \(\sigma_1 \geq \cdots \geq \sigma_r > 0\).
    \subsection{Reduced SVD}
    Ignores \(m - n\) ``0'' rows in \(\symbf{\Sigma}\) so that \(\symbf{U} \in \R^{m \times n}\) and \(\symbf{\Sigma} \in \R^{n \times n}\).
    \subsection{Pseudoinverse}
    Consider the inverse mapping \(\symbfit{u}_i \mapsto \frac{1}{\sigma_i} \symbfit{v}_i\)
    \begin{equation*}
        \symbf{A}^\dagger \symbfit{u}_i = \frac{1}{\sigma_i} \symbfit{v}_i
        \iff
        \symbf{A}^\dagger \symbfit{u}_i = \frac{1}{\sigma_i} \symbfit{v}_i \symbfit{u}^\top_i \symbfit{u}_i
    \end{equation*}
    \begin{equation*}
        \symbf{A}^\dagger = \sum_{i = 1}^r \frac{1}{\sigma_i} \symbfit{v}_i \symbfit{u}^\top_i
        \iff
        \symbf{A}^\dagger = \symbf{V} \symbf{\Sigma}^\dagger \symbf{U}^\top
    \end{equation*}
    where \(\symbf{\Sigma}^\dagger = \diag{\left( \sigma_1,\: \dots,\: \sigma_r,\: 0,\: \dots,\: 0 \right)}\).
    \(\symbfit{x} = \symbf{A}^\dagger \symbfit{b}\) solves LS\@.
    \subsection{Truncated SVD}
    Express \(\symbf{A}\) as the sum of rank-1 matrices:
    \begin{align*}
        \symbf{A} & = \sum_{i = 1}^n \sigma_i \symbfit{u}_i \symbfit{v}^\top_i                                  \\
        \symbf{A} & = \sum_{i = 1}^r \sigma_i \symbfit{u}_i \symbfit{v}^\top_i &  & (\sigma_{r < i \leq n} = 0)
    \end{align*}
    Rank-\(\nu\) approximation of \(\symbf{A}\):
    \begin{equation*}
        \symbf{A} \approx \tilde{\symbf{A}} = \sum_{i = 1}^\nu \sigma_i \symbfit{u}_i \symbfit{v}^\top_i.
    \end{equation*}
    \section{General Vector Spaces}
    \(V\) is a vector space with vectors \(\symbfit{v} \in V\) if the following 10 axioms are satisfied
    for \(\forall \symbfit{u}, \symbfit{v}, \symbfit{w} \in V\) and \(\forall k, m \in \R\),
    given an addition and scalar multiplication operation.

    \underline{For the addition operation:}
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item Closure: \(\symbfit{u} + \symbfit{v} \in V\)
        \item Commutativity: \(\symbfit{u} + \symbfit{v} = \symbfit{v} + \symbfit{u} \in V\)
        \item Associativity: \begin{equation*}\symbfit{u} + \left( \symbfit{v} + \symbfit{w} \right) = \left( \symbfit{u} + \symbfit{v} \right) + \symbfit{w}\end{equation*}
        \item Identity: \(\exists \symbfup{0} \in V : \symbfit{u} + \symbfup{0} = \symbfup{0} + \symbfit{u} = \symbfit{u}\)
        \item Inverse: \(\exists \left( -\symbfit{u} \right) \in V : \symbfit{u} + \left( -\symbfit{u} \right) = \symbfup{0}\)
    \end{itemize}
    \underline{For the scalar multiplication operation:}
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item Closure: \(k \symbfit{u} \in V\)
        \item Distributivity: \(k \left( \symbfit{u} + \symbfit{v} \right) = k\symbfit{u} + k\symbfit{v}\)
        \item Distributivity: \(\left( k + m \right) \symbfit{u} = k\symbfit{u} + m\symbfit{u}\)
        \item Associativity: \(k \left( m\symbfit{u} \right) = \left( k m \right) \symbfit{u}\)
        \item Identity: \(\exists 1 \in \R : 1 \symbfit{u} = \symbfit{u}\)
    \end{itemize}

    \subsection{Examples of Vector Spaces}
    The set of \(m \times n\) matrices \(\mathscr{M}_{mn}\) with matrix addition and scalar multiplication.

    The set of functions \(\mathscr{F}\left( \Omega \right) : \Omega \to \R\) with addition and scalar multiplication defined pointwise.
    \subsection{Subspaces}
    The subset \(W \subset V\) is itself a vector space if it is closed under addition and scalar multiplication.
    \subsection{Examples of Subspaces}
    \underline{Subspaces of \(\R^n\):}
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item Lines, planes and higher-dimensional analogues in \(\R^n\) \emph{passing through the origin}.
    \end{itemize}
    \underline{Subspaces of \(\mathscr{M}_{nn}\):}
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item The set of all \emph{symmetric} \(n \times n\) matrices, denoted \(\mathscr{S}_n \subset \mathscr{M}_{nn}\).
        \item The set of all \emph{skew symmetric} \(n \times n\) matrices, denoted \(\mathscr{K}_n \subset \mathscr{M}_{nn}\).
    \end{itemize}
    \underline{Subspaces of \(\mathscr{F}\):}
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item The set of all \emph{polynomials} of degree \(n\) or less, denoted \(\mathscr{P}_n\left( \Omega \right) \subset \mathscr{F}\left( \Omega \right)\).
        \item The set of all \emph{continuous functions}, denoted \(C\left( \Omega \right) \subset \mathscr{F}\left( \Omega \right)\).
        \item The set of all functions with \emph{continuous \(n\)th derivatives}, denoted \(C^n\left( \Omega \right) \subset C\left( \Omega \right)\).
        \item The set of all functions \(f\) defined on \(\interval{0}{1}\) satisfying \(f\left( 0 \right) = f\left( 1 \right)\).
    \end{itemize}
    \subsection{General Vector Space Terminology}
    Let \(S = \left\{ \symbfit{v}_1,\: \dots,\: \symbfit{v}_k \right\}\) and \(c_1,\: \dots,\: c_k \in \R\):
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item The linear combination of \(S\)
              is a vector of the form \(\symbfit{v} = c_1 \symbfit{v}_1 + \cdots + c_k \symbfit{v}_k\).
        \item \(S\) is linearly independent iff
              \(c_1 \symbfit{v}_1 + \cdots + c_k \symbfit{v}_k = \symbfup{0}\) has the trivial solution.
        \item \(\vspan{\left( S \right)}\) is the set of all linear combinations of \(S\).
    \end{itemize}
    \(S\) is a \textit{basis} for a vector space \(V\) if
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item \(S\) is linearly independent.
        \item \(\vspan{\left( S \right)} = V\).
    \end{itemize}
    The number of basis vectors denotes the dimension of \(V\).

    Not all vector spaces have a basis. Function spaces such as \(C\)
    are infinite dimensional.
    \subsection{Examples of Standard Bases}
    \begin{itemize}[leftmargin=*,itemsep=-1ex,partopsep=1ex,parsep=1ex]
        \item \(\mathscr{M}_{22}\):
              \begin{equation*}
                  \left\{ \begin{bmatrix*}
                      1 & 0 \\
                      0 & 0
                  \end{bmatrix*},\: \begin{bmatrix*}
                      0 & 0 \\
                      1 & 0
                  \end{bmatrix*},\: \begin{bmatrix*}
                      0 & 1 \\
                      0 & 0
                  \end{bmatrix*},\: \begin{bmatrix*}
                      0 & 0 \\
                      0 & 1
                  \end{bmatrix*} \right\}
              \end{equation*}
        \item \(\mathscr{S}_{22}\): \(\left\{ \begin{bmatrix*}
                  1 & 0 \\
                  0 & 0
              \end{bmatrix*},\: \begin{bmatrix*}
                  0 & 1 \\
                  1 & 0
              \end{bmatrix*},\: \begin{bmatrix*}
                  0 & 0 \\
                  0 & 1
              \end{bmatrix*} \right\}\)
        \item \(\mathscr{K}_{22}\): \(\left\{ \begin{bmatrix*}
                  0 & 1 \\
                  -1 & 0
              \end{bmatrix*}\right\}\)
        \item \(\mathscr{P}_3\): \(\left\{ 1,\: x,\: x^2,\: x^3 \right\}\)
    \end{itemize}
    \subsection{Linear Transformations}
    \(T:V \to W\) satisfying
    \begin{align*}
        T\left( k\symbfit{u} \right) & = k T\left( \symbfit{u} \right) \\
        T\left( \symbfit{u} + \symbfit{v} \right) & = T\left( \symbfit{u} \right) + T\left( \symbfit{v} \right)
    \end{align*}
    % These defining properties allow us to characterise a linear transformation completely
    % by considering how the basis vectors from \(V\) map to \(W\). Any vector \(\symbfit{v} \in V\)
    % can be written in terms of a basis \(B = \left\{ \symbfit{v}_1,\: \dots,\: \symbfit{v}_n \right\}\), such that
    % \begin{equation*}
    %     \symbfit{v} = x_1 \symbfit{v}_1 + \cdots + x_n \symbfit{v}_n.
    % \end{equation*}
    % Therefore
    % \begin{align*}
    %     T\left( \symbfit{v} \right) & = T\left( x_1 \symbfit{v}_1 + \cdots + x_n \symbfit{v}_n \right)                 \\
    %                                 & = T\left( x_1 \symbfit{v}_1 \right) + \cdots + T\left( x_n \symbfit{v}_n \right) \\
    %                                 & = x_1 T\left( \symbfit{v}_1 \right) + \cdots + x_n T\left( \symbfit{v}_n \right)
    % \end{align*}
    % Now consider the coordinate vector of \(\symbfit{w} \in W\) relative to the basis \(B' = \left\{ \symbfit{w}_1,\: \dots,\: \symbfit{w}_m \right\}\) is
    % \begin{align*}
    %     \symbfit{w} & = b_1 \symbfit{w}_1 + \cdots + b_m \symbfit{w}_m \\
    %                 & = B' \symbfit{b}                                 \\
    %                 & = B' \left( \symbfit{w} \right)_{B'}
    % \end{align*}
    % so that the linear transformation can be expressed as follows
    % \begin{align*}
    %     T\left( \symbfit{v} \right)                                                                                       & = \symbfit{w}                                    \\
    %     x_1 T\left( \symbfit{v}_1 \right) + \cdots + x_n T\left( \symbfit{v}_n \right)                                    & = b_1 \symbfit{w}_1 + \cdots + b_m \symbfit{w}_m \\
    %     \begin{bmatrix}
    %         \vertbar                      &        & \vertbar                      \\
    %         T\left( \symbfit{v}_1 \right) & \cdots & T\left( \symbfit{v}_n \right) \\
    %         \vertbar                      &        & \vertbar
    %     \end{bmatrix} \symbfit{x}                                         & = B' \symbfit{b}                                                                                 \\
    %     \begin{bmatrix}
    %         \vertbar                                          &        & \vertbar                                          \\
    %         \left( T\left( \symbfit{v}_1 \right) \right)_{B'} & \cdots & \left( T\left( \symbfit{v}_n \right) \right)_{B'} \\
    %         \vertbar                                          &        & \vertbar
    %     \end{bmatrix} \symbfit{x} & = \symbfit{b}                                                       \\
    %     \symbf{A} \symbfit{x}                                                                                             & = \symbfit{b}                                    \\
    %     \left( T \right)_{B',\: B} \left( \symbfit{v} \right)_B                                                           & = \left( \symbfit{w} \right)_{B'}.
    % \end{align*}
    % Therefore the linear transformation between the vector spaces \(V\) and \(W\)
    % can be represented as the transformation of coordinate vectors relative to the bases \(B\) and \(B'\), denoted \(\left( T \right)_{B',\: B}\), that is, the matrix \(\symbf{A}\).
    % \begin{definition}[Isomorphism]
    %     A linear transformation \(T : V \to W\) is an isomorphism between \(V\) and \(W\) if there exists a bijection between the two vector spaces.

    %     All \(n\) dimensional vector spaces \(V\) are isomorphic to \(\R^n\). This is a result of the coordinate vectors of \(V\) with respect
    %     to the basis \(B\) that allow us to represent each vector \(\symbfit{v} \in V\) as a linear combination of the standard basis vectors in \(\R^n\).
    % \end{definition}
    % \subsection{Fundamental Subspaces of \texorpdfstring{\(T\)}{T}}
    % The four fundamental subspaces also generalise to arbitrary linear transformations.

    % Given the linear transformation \(T:V \to W\):
    % \begin{itemize}
    %     \item The set of all vectors in \(V\) that map to \(W\) is the \textbf{image} of \(T\), denoted \(\vim{\left( T \right)}\).
    %     \item The set of all vectors in \(W\) that is mapped to by a vector in \(V\) is the \textbf{range} of \(T\), denoted \(\vrange{\left( T \right)}\).
    %     \item The set of all vectors in \(V\) that \(T\) maps to \(\symbfup{0}_W\) is the \textbf{kernel} of \(T\), denoted \(\vker{\left( T \right)}\).
    % \end{itemize}
    % If the range of \(T\) is finite-dimensional, its dimension is the \textbf{rank} of \(T\), and
    % if the kernel of \(T\) is finite-dimensional, its dimension is the \textbf{nullity} of \(T\), so
    % that the rank-nullity theorem continues to hold.
    % \begin{equation*}
    %     \vrank{\left( T \right)} + \nullity{\left( T \right)} = \dim{\left( V \right)}.
    % \end{equation*}
    % \subsection{Inner Product Spaces}
    % To describe geometric properties of vector spaces, we introduce an operation called
    % the inner product that acts on the vectors in a vector space. The inner product associates a pair of vectors
    % to a real number, and is delimited by angle brackets
    % \begin{equation*}
    %     \abracket*{\cdot,\: \cdot} : V \times V \to \R.
    % \end{equation*}
    % This operation must satisfy the following axioms. For \(\symbfit{u},\: \symbfit{v},\: \symbfit{w} \in V\)
    % and \(k \in \R\):
    % \begin{table}[H]
    %     \centering
    %     \begin{tabular}{c c}
    %         \toprule
    %         \textbf{Axiom}                  & \textbf{Meaning}                                                                                                                           \\
    %         \midrule
    %         Symmetry                        & \(\abracket*{\symbfit{u},\: \symbfit{v}} = \abracket*{\symbfit{v},\: \symbfit{u}}\)                                                        \\
    %         Linearity in the first argument & \(\abracket*{\symbfit{u} + \symbfit{v},\: \symbfit{w}} = \abracket*{\symbfit{u},\: \symbfit{w}} + \abracket*{\symbfit{v},\: \symbfit{w}}\) \\
    %         Linearity in the first argument & \(\abracket*{k \symbfit{u},\: \symbfit{v}} = k\abracket*{\symbfit{u},\: \symbfit{v}}\)                                                     \\
    %         Positive semi-definitiveness    & \(\abracket*{\symbfit{u},\: \symbfit{u}} \geq 0\), \(\abracket*{\symbfit{u},\: \symbfit{u}} = 0 \iff \symbfit{u} = \symbfup{0}\)           \\
    %         \bottomrule
    %     \end{tabular}
    %     \caption{Inner product axioms.} % \label{}
    % \end{table}
    % A vector space that defines such an operation called an inner product space.
    % \subsubsection{Examples of Inner Products}
    % In \(\R^n\):
    % \begin{itemize}
    %     \item \(\abracket*{\symbfit{u},\: \symbfit{v}} = \symbfit{u} \cdot \symbfit{v} = \symbfit{u}^\top \symbfit{v}\). This is the standard inner product called the ``dot product''.
    %     \item \(\abracket*{\symbfit{u},\: \symbfit{v}} = \symbfit{u}^\top \symbf{A} \symbfit{v}\) where \(\symbf{A}\) is positive definite. This is a weighted inner product, which can be used in weighted least squares.
    % \end{itemize}
    % For matrices \(\symbf{A},\: \symbf{B} \in \mathscr{M}_{mn}\), the standard inner product is defined
    % \begin{equation*}
    %     \abracket*{\symbf{A},\: \symbf{B}} = \Tr{\left( \symbf{A}^\top \symbf{B} \right)}.
    % \end{equation*}
    % For continuous function spaces, consider \(f,\: g \in C\left( \interval{a}{b} \right)\) where
    % the inner product operation is defined by the integral
    % \begin{equation*}
    %     \abracket*{f,\: g} = \int_a^b f\left( x \right) g\left( x \right) \odif{x}.
    % \end{equation*}
    % As the vector spaces \(\mathscr{M}_{mn}\) and \(\mathscr{P}_n\) are isomorphic to \(\R^{mn}\) and \(\R^{n + 1}\) respectively,
    % we can use the inner product definitions from \(\R^n\) for these spaces also.

    % Likewise we can also consider the following integral definition with a continuous weight function \(w\left( x \right)\) that is positive for all \(x \in \interval{a}{b}\):
    % \begin{equation*}
    %     \abracket*{f,\: g} = \int_a^b f\left( x \right) g\left( x \right) w\left( x \right) \odif{x}.
    % \end{equation*}
    % \subsubsection{Norms}
    % Having defined an inner product, we can also define the norm as
    % \begin{equation*}
    %     \norm*{\symbfit{v}} = \sqrt{\abracket*{\symbfit{v},\: \symbfit{v}}}
    % \end{equation*}
    % From this definition, we maintain the expected properties of the Euclidean norm:
    % \begin{itemize}
    %     \item \(\norm*{\symbfit{v}} \geq 0\) and \(\norm*{\symbfit{v}} = 0\) iff \(\symbfit{v} = \symbfup{0}\).
    %     \item \(\norm*{k \symbfit{v}} = \abs*{k} \norm*{\symbfit{v}}\) for \(k \in \R\).
    %     \item \(\norm*{\symbfit{u} + \symbfit{v}} \leq \norm*{\symbfit{u}} + \norm*{\symbfit{v}}\), which is the triangle inequality.
    % \end{itemize}
    % For matrices, the inner product inherited from \(\R^{mn}\) leads to the following definitions of norms:
    % \begin{equation*}
    %     \norm*{\symbf{A}} = \sqrt{\abracket*{\symbf{A},\: \symbf{A}}} = \sqrt{\Tr{\left( \symbf{A}^\top \symbf{A} \right)}} = \sqrt{\sum_{i = 1}^m \sum_{j = 1}^n a_{ij}^2}
    % \end{equation*}
    % which is known as the Frobenius norm. For continuous functions \(f \in C\left( \interval{a}{b} \right)\):\
    % \begin{equation*}
    %     \norm*{f\left( x \right)} = \sqrt{\abracket*{f\left( x \right),\: f\left( x \right)}} = \sqrt{\int_a^b f\left( x \right)^2 \odif{x}}.
    % \end{equation*}
    % \subsubsection{Orthogonality}
    % Similarly, we can say that two vectors \(\symbfit{u}\) and \(\symbfit{v}\) are orthogonal if
    % \begin{equation*}
    %     \abracket*{\symbfit{v},\: \symbfit{v}} = 0.
    % \end{equation*}
    % Using this definition we can show that all
\end{multicols*}
\end{document}