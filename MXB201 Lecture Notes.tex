%!TEX program = xelatex
\documentclass{article}
\usepackage{LaTeX-Submodule/template}

% Additional packages & macros
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Header and footer
\newcommand{\unitName}{Advanced Linear Algebra}
\newcommand{\unitTime}{Semester 1, 2022}
\newcommand{\unitCoordinator}{Prof Timothy Moroney}
\newcommand{\documentAuthors}{\textsc{Tarang Janawalkar}}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Fundamental Concepts of Linear Algebra}
\subsection{Row Echelon Form}
As studied in Linear Algebra, we can solve linear systems by
applying the following elementary row operations to any matrix \(\symbf{A}\).
\begin{enumerate}[label=Type \Roman*.]
    \item Exchange any two rows.
    \item Multiply any row by a constant.
    \item Add a multiple of one row to another row.
\end{enumerate}
This allows us to reduce \(\symbf{A}\) into \textbf{row echelon form}
such that the entries below the main diagonal are zero:
\begin{equation*}
    \symbf{R}_{\mathrm{ref}} =
    \begin{bmatrix}
        r_{11} & r_{12} & \cdots & r_{1n} \\
               & r_{22} & \cdots & r_{2n} \\
               &        & \ddots & \vdots \\
               &        &        & r_{mn}
    \end{bmatrix}
\end{equation*}
\subsection{Elementary Matrix}
Mathematically, we can represent these row operations as a matrix
which is left multiplied to \(\symbf{A}\).
\begin{definition}[Elementary matrix]
    An elementary matrix \(\symbf{E}_i\) is constructed by applying a row operation to the elementary matrix \(\symbf{I}_m\).
    Consider a 3 by 4 matrix \(\symbf{A}\); a common first elementary row operation might be
    \begin{equation*}
        r_2 \leftarrow r_2 - \frac{a_{21}}{a_{11}} r_1
    \end{equation*}
    which when applied to \(\symbf{I}_3\) yields
    \begin{equation*}
        \symbf{E}_1 =
        \begin{bmatrix}
            1                      & 0 & 0 \\
            -\frac{a_{21}}{a_{11}} & 1 & 0 \\
            0                      & 0 & 1
        \end{bmatrix}
    \end{equation*}
    where the 1 subscript simply indicates the first of many elementary row operations.
    Left multiplying this to an arbitrary \(\symbf{A}\) gives
    \begin{align*}
        \symbf{E}_1 \symbf{A} & =
        \begin{bmatrix}
            1                      & 0 & 0 \\
            -\frac{a_{21}}{a_{11}} & 1 & 0 \\
            0                      & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            a_{11} & a_{12} & a_{13} & a_{14} \\
            a_{21} & a_{22} & a_{23} & a_{24} \\
            a_{31} & a_{32} & a_{33} & a_{34}
        \end{bmatrix} \\
                              & =
        \begin{bmatrix}
            a_{11} & a_{12}                              & a_{13}                              & a_{14}                              \\
            0      & a_{22}-\frac{a_{12} a_{21}}{a_{11}} & a_{23}-\frac{a_{13} a_{21}}{a_{11}} & a_{24}-\frac{a_{14} a_{21}}{a_{11}} \\
            a_{31} & a_{32}                              & a_{33}                              & a_{34}
        \end{bmatrix}
    \end{align*}
    which has the desired result of eliminating the first column of the second row.
\end{definition}
\subsection{Reduced Row Echelon Form}
As there are infinitely many ways to reduce a matrix to row echelon form, we typically
reduce \(\symbf{R}_{\mathrm{ref}}\) further into \textbf{reduced row echelon form} which is
a unique reduction for every \(\symbf{A}\).

This matrix \(\symbf{R}_{\mathrm{rref}}\) (or simply \(\symbf{R}\)) generally requires \(m \times n\)
elementary row operations and is only useful for theoretical analysis.
In reduced row echelon form, any entries in the same column as a pivot must be 0, and each pivot is 1.
\subsection{Elimination Matrix}
The elementary matrices involved in row reduction can be expressed as a single matrix containing every
each row operation.
\begin{align*}
    \symbf{E}_9 \symbf{E}_8 \dots \symbf{E}_2 \symbf{E}_1 \symbf{A} & = \symbf{E} \symbf{A} \\
                                                                    & = \symbf{R}
\end{align*}
\subsection{Linear Systems}
Given the linear system \(\symbf{A} \symbfit{x} = \symbfit{b}\)
we can augment \(\symbf{A}\) with \(\symbfit{b}\) to draw conclusions about the solutions.

If we left multiply the elimination matrix \(\symbf{E}\) to
\(\begin{bmatrix}[c|c]
    \symbf{A} & \symbfit{b}
\end{bmatrix}\)
we can apply the same operations to \(\symbfit{b}\).
\begin{align*}
    \symbf{E}
    \begin{bmatrix}[c|c]
        \symbf{A} & \symbfit{b}
    \end{bmatrix} & =
    \begin{bmatrix}[c|c]
        \symbf{E} \symbf{A} & \symbf{E} \symbfit{b}
    \end{bmatrix}          \\
                             & = \begin{bmatrix}[c|c]
                                     \symbf{R} & \symbfit{z}
                                 \end{bmatrix}
\end{align*}
Therefore
\begin{equation*}
    \symbf{R} \symbfit{x} = \symbfit{z}
\end{equation*}
After reducing the matrix \(\symbf{A}\) to \(\symbf{R}\), we can summarise
certain characteristics about \(\symbf{A}\).
\subsubsection{Basic and Free Variables}
Identifying the pivots in \(\symbf{R}\) allows us to determine
the dimensions of various subspaces of \(\symbf{A}\).
\begin{definition}[Basic variables]
    The columns that a pivot corresponds to are known as basic variables (or leading variables).
\end{definition}
\begin{definition}[Free variables]
    Any columns not corresponding to any pivots are known as free variables (or parameters).
    Consequently, any variables that are not basic variables are free variables.
\end{definition}
When using backward substitution to solve \(\symbf{R} \symbfit{x} = \symbf{z}\), we assign new variables
to any free variables to indicate that they are parameters to the system.
\subsubsection{Singular Matrices}
An \(n\) by \(n\) square matrix \(\symbf{A}\) is singular if its associated reduced matrix \(\symbf{R}\) has fewer than \(n\) basic variables.
It follows that a singular matrix also has a determinant of 0 (as the product of the diagonal is 0) which means it is also noninvertible.
\subsection{The Four Fundamental Subspaces}
Consider
\begin{align*}
    \symbf{A} & =
    \begin{bmatrix}
        1 & 2 & 3 & 4 & 5 \\
        0 & 0 & 1 & 4 & 5 \\
        0 & 0 & 0 & 0 & 0
    \end{bmatrix} \\
    \symbf{R} & =
    \begin{bmatrix}
        1 & 2 & 0 & -2 & -1 \\
        0 & 0 & 1 & 2  & 2  \\
        0 & 0 & 0 & 0  & 0
    \end{bmatrix}, \,
    \symbf{E} =
    \begin{bmatrix}
        1 & -3 & 0 \\
        0 & 1  & 0 \\
        0 & 0  & 1
    \end{bmatrix}
\end{align*}
so that \(x_1\) and \(x_3\) are basic variables, whereas \(x_2\), \(x_4\) and \(x_5\) are free variables.
\subsubsection{Row space}
The rows containing pivots form the basis vectors for the rowspace of \(\symbf{A}\), denoted \(\rowspace{A}\).
\begin{equation*}
    \rowspace{A} =
    \vspan{\left\{
        \begin{bmatrix}
            1  \\
            2  \\
            0  \\
            -2 \\
            -1
        \end{bmatrix},\,
        \begin{bmatrix}
            0 \\
            0 \\
            1 \\
            2 \\
            2
        \end{bmatrix}
        \right\}}
\end{equation*}
Note that the row vectors here are represented as column vectors
to allows us to conveniently compare these vectors with other spaces.
\subsubsection{Null space}
The span of vectors that satisfy the homogeneous system \(\symbf{A}\symbfit{x} = \symbf{0}\), form the null space of \(\symbf{A}\), denoted \(\nullspace{A}\).
\begin{equation*}
    \nullspace{A} =
    \vspan{\left\{
        \begin{bmatrix}
            -2 \\
            1  \\
            0  \\
            0  \\
            0
        \end{bmatrix},\,
        \begin{bmatrix}
            2  \\
            0  \\
            -2 \\
            1  \\
            0
        \end{bmatrix},\,
        \begin{bmatrix}
            1  \\
            0  \\
            -2 \\
            0  \\
            1
        \end{bmatrix}
        \right\}}
\end{equation*}
\subsubsection{Column space}
By considering a general vector \(\symbfit{b}\), we can construct the column space of \(\symbf{A}\), denoted \(\columnspace{A}\).
This is done by augmenting \(\symbf{A}\) with \(\symbfit{b}\), and appling the same elimination matrix \(\symbf{E}\).
\begin{equation*}
    \symbf{E}
    \begin{bmatrix}[c|c]
        \symbf{A} & \symbfit{b}
    \end{bmatrix} =
    \begin{bmatrix}[ccccc|c]
        1 & 2 & 0 & -2 & -1 & b_1 - 3b_2 \\
        0 & 0 & 1 & 2  & 2  & b_2        \\
        0 & 0 & 0 & 0  & 0  & b_3
    \end{bmatrix}
\end{equation*}
here we determine any constraints required to make \(\symbf{A}\symbfit{x} = \symbfit{b}\)
consistent, resulting in
\begin{equation*}
    \symbfit{b} =
    \begin{bmatrix}
        b_1 \\
        b_2 \\
        0
    \end{bmatrix}
\end{equation*}
This vector will guarantee a consistent solution for any \(b_1\) and \(b_2\), with \(b_3 = 0\).

Rewriting \(\symbfit{b}\) in terms of its two parameters \(b_1\) and \(b_2\),
we can construct the basis vectors for the column space of \(\symbf{A}\).
\begin{equation*}
    \columnspace{A} =
    \vspan{\left\{
        \begin{bmatrix}
            1 \\
            0 \\
            0
        \end{bmatrix},\,
        \begin{bmatrix}
            0 \\
            1 \\
            0
        \end{bmatrix}
        \right\}}
\end{equation*}
Note that we could have also calculated the column space by finding the rowspace of \(\symbf{A}^\top\).
\subsubsection{Left-Null Space}
Just as we found the null space for \(\symbf{A}\), we can find a null space for \(\symbf{A}^\top\).
This is known as the left-null space, denoted \(\leftnullspace{A}\).
\begin{equation*}
    \leftnullspace{A} =
    \vspan{\left\{
        \begin{bmatrix}
            0 \\
            0 \\
            1
        \end{bmatrix}
        \right\}}
\end{equation*}
These four spaces form the fundamental subspaces for any matrix \(\symbf{A}\).
\subsubsection{Dimensions of Subspaces}
For the matrix \(\symbf{A} \in \R^{m \times n}\):
\begin{definition}[Rank]
    The dimension of the row space is called the \textbf{rank} of a matrix.
    \begin{equation*}
        \vrank{\left( \symbf{A} \right)} = \dim{\left( \rowspace{A} \right)} = r
    \end{equation*}
    To determine the rank, we can count the number of basic variables in \(\symbf{R}\).

    Note that the \(\vrank{\left( \symbf{A} \right)} = \vrank{\left( \symbf{A}^\top \right)}\).
\end{definition}
\begin{definition}[Nullity]
    The dimension of the null space is called the \textbf{nullity} of the matrix.
    \begin{equation*}
        \vnull{\left( \symbf{A} \right)} = \dim{\left( \nullspace{A} \right)}
    \end{equation*}
\end{definition}
\begin{definition}[Left nullity]
    The dimension of the left null space is called the \textbf{left nullity} of the matrix.
    \begin{equation*}
        \vnull{\left( \symbf{A}^\top \right)} = \dim{\left( \leftnullspace{A} \right)}
    \end{equation*}
\end{definition}
\begin{theorem}[Rank-nullity theorem]
    The dimension of the domain of \(\symbf{A}\), \(\R^n\), is given by
    the sum of the dimensions of the row space and null space of \(\symbf{A}\).
    \begin{align*}
        \dim{\left( \rowspace{A} \right)} + \dim{\left( \nullspace{A} \right)} & = \dim{\left( \R^n \right)} \\
        \vrank{\left( \symbf{A} \right)} + \vnull{\left( \symbf{A} \right)}    & = n                         \\
        r + \vnull{\left( \symbf{A} \right)}                                   & = n
    \end{align*}
    Therefore
    \begin{equation*}
        \vnull{\left( \symbf{A} \right)} = n - r
    \end{equation*}
\end{theorem}
\begin{corollary}[Rank-nullity theorem for the transpose]
    The dimension of the codomain of \(\symbf{A}\), \(\R^m\), is given by
    the sum of the dimensions of the column space and left-null space of \(\symbf{A}\).
    \begin{align*}
        \dim{\left( \columnspace{A} \right)} + \dim{\left( \leftnullspace{A} \right)} & = \dim{\left( \R^m \right)} \\
        \vrank{\left( \symbf{A}^\top \right)} + \vnull{\left( \symbf{A}^\top \right)} & = m                         \\
        r + \vnull{\left( \symbf{A}^\top \right)}                                     & = m
    \end{align*}
    Therefore
    \begin{equation*}
        \vnull{\left( \symbf{A}^\top \right)} = m - r
    \end{equation*}
\end{corollary}
\begin{theorem}[Orthogonality of subspaces]
    The row space and null space are orthogonal complements in \(\R^n\).
    \begin{equation*}
        \rowspace{A}^\perp = \nullspace{A}
    \end{equation*}
    Similarly, the column space and left-null space are orthogonal complements in \(\R^m\).
    \begin{equation*}
        \columnspace{A}^\perp = \leftnullspace{A}
    \end{equation*}
\end{theorem}
\subsection{Consistency of a Linear System}
Given a matrix \(\symbf{A} \in \R^{m \times n}\), the linear system \(\symbf{A}\symbfit{x} = \symbfit{b}\),
with any vector \(\symbfit{b}\) can be described as follows:
\begin{enumerate}
    \item Consistent with unique solution:
          \begin{equation*}
              \vrank{\left( \symbf{A} \right)} =
              \vrank{\left( \begin{bmatrix}[c|c]
                      \symbf{A} & \symbfit{b}
                  \end{bmatrix} \right)} = n
          \end{equation*}
    \item Consistent with infinitely many solutions:
          \begin{equation*}
              \vrank{\left( \symbf{A} \right)} =
              \vrank{\left( \begin{bmatrix}[c|c]
                      \symbf{A} & \symbfit{b}
                  \end{bmatrix} \right)}
          \end{equation*}
    \item Inconsistent with no solutions:
          \begin{equation*}
              \vrank{\left( \symbf{A} \right)} \neq
              \vrank{\left( \begin{bmatrix}[c|c]
                      \symbf{A} & \symbfit{b}
                  \end{bmatrix} \right)}
          \end{equation*}
\end{enumerate}
However, if \(\symbfit{b} \in \columnspace{{A}}\), the system must be consistent.
\subsection{General Solution to a Linear System}
Given \(\symbfit{b} \in \columnspace{A}\), the general solution to a system can be expressed as
\begin{equation*}
    \symbfit{x}_g = \symbfit{x}_p + \symbfit{x}_n
\end{equation*}
where \(\symbfit{x}_p\) is a particular solution obtained by backward substitution, and
\(\symbfit{x}_n\) represents the linear combination of all null space basis vectors.
\subsection{Minimum Norm Solution}
In general, the particular solution may contain a linear combination of
null space basis vectors requiring \(\symbfit{x}_p \in \R^n\).

If we consider the solution vector \(\symbfit{x}_r \in \rowspace{A}\), then this vector
will be the minimum norm solution to \(\symbf{A}\symbfit{x} = \symbfit{b}\).
\section{Least Squares}
Given an inconsistent system \(\symbf{A}\symbfit{x} = \symbfit{b}\), we can consider an approximate solution such that we
minimise the norm of the residual:
\begin{equation*}
    \norm{\symbfit{b} - \symbf{A}\symbfit{x}}
\end{equation*}
Therefore we solve the following minimisation problem
\begin{equation*}
    \symbfit{x} = \argmin_{\symbfit{x}^\ast \in \R^n} \norm{\symbfit{b} - \symbf{A}\symbfit{x}}
\end{equation*}
which is known as the \textbf{Least Squares} problem.
\begin{theorem}[Minimum norm solution]
    The solution to the least squares problem is obtained by the vector \(\symbfit{x}\) such
    that \(\symbfit{b} - \symbf{A}\symbfit{x}\) is orthogonal to the column space of \(\symbf{A}\).
\end{theorem}
\subsection{Normal Equations}
Given that \(\symbfit{b} - \symbf{A}\symbfit{x}\) is orthogonal to \(\columnspace{A}\), it must lie in the
left-null space of \(\symbfit{A}\). By orthogonality, we form the following relationship
\begin{equation*}
    \symbf{A}^\top \left( \symbfit{b} - \symbf{A}\symbfit{x} \right) = \symbfit{0}
\end{equation*}
which is equivalent to solving
\begin{equation*}
    \symbf{A}^\top \symbf{A}\symbfit{x} = \symbf{A}^\top \symbfit{b}.
\end{equation*}
This linear equation is known as the \textbf{Normal Equations}, which can be thought of as
a \linebreak generalisation of \(\symbf{A}\symbfit{x} = \symbfit{b}\). But whereas \(\symbf{A}\symbfit{x} = \symbfit{b}\)
can be inconsistent, the normal equations are always consistent.
\subsection{Orthogonal Projection}
By rearranging the normal equations, we can directly compute the solution vector.
\begin{equation*}
    \symbfit{x} = \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top \symbfit{b}
\end{equation*}
The projection of \(\symbfit{b}\) onto \(\columnspace{A}\) is therefore
\begin{align*}
    \symbfit{b}_p & = \proj_{\columnspace{A}} \left( \symbfit{b} \right)                                \\
                  & = \symbf{A} \symbfit{x}                                                             \\
                  & = \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top \symbfit{b}
\end{align*}
We can identify the matrix
\begin{equation*}
    \symbf{P} = \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top
\end{equation*}
as the orthogonal projector onto the column space of \(\symbf{A}\), so that it operates on \(\symbfit{b}\):
\begin{equation*}
    \symbf{P} \symbfit{b} = \proj_{\columnspace{A}} \left( \symbfit{b} \right)
\end{equation*}
\begin{definition}[Idempotent]
    A matrix \(\symbf{P}\) is idempotent if it satisfies
    \begin{equation*}
        \symbf{P}^2 = \symbf{P}
    \end{equation*}
\end{definition}
\begin{theorem}
    Given that \(\symbf{P}\symbfit{b}\) produces the projection vector \(\symbfit{b}_p\),
    taking the orthogonal projection of \(\symbfit{b}_p\) again results in the vector \(\symbfit{b}_p\).
    \begin{align*}
        \symbf{P} \symbfit{b}_p        & = \symbfit{b}_p         \\
        \symbf{P}\symbf{P} \symbfit{b} & = \symbf{P} \symbfit{b} \\
        \symbf{P}^2 \symbfit{b}        & = \symbf{P} \symbfit{b}
    \end{align*}
    Therefore orthogonal projectors are idempotent matrices as \(\symbf{P}^2 = \symbf{P}\).
\end{theorem}
\begin{proof}
    If we express the orthogonal projector in its full form, we can verify the result from above
    \begin{align*}
        \symbf{P}^2 & = \symbf{P} \symbf{P}                                                                                                                                           \\
                    & = \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top                   \\
                    & = \symbf{A} \cancel{\left( \symbf{A}^\top \symbf{A} \right)^{-1}} \cancel{\symbf{A}^\top \symbf{A}} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top \\
                    & = \symbf{A} \left( \symbf{A}^\top \symbf{A} \right)^{-1} \symbf{A}^\top                                                                                         \\
                    & = \symbf{P}
    \end{align*}
\end{proof}
\subsection{Dependent Columns}
If \(\symbf{A}\) has dependent columns, then we will obtain an infinite family of Least
Squares solutions. However there will only be one projection vector \(\symbfit{b}_p\).

This arises from \(\nullspace{A} = \mathcal{N}\left(\symbf{A}^\top \symbf{A}\right)\), so that
the solution to the Normal Equations yields
\begin{equation*}
    \symbfit{x} = \symbfit{x}_p + \symbfit{x}_n
\end{equation*}
where \(\symbfit{x}_p\) is the particular Least Squares solution, and \(\symbfit{x}_n\) represents
any linear combination of null space vectors.
\subsection{Orthogonal Projector onto the Column Space}
To obtain a unique solution to the Normal Equations, we can form the matrix \(\symbf{W}\)
such that it has full column rank and it spans the column space of \(\symbf{A}\).
Then the orthogonal projector is given by
\begin{equation*}
    \symbf{P} = \symbf{W} \left( \symbf{W}^\top \symbf{W} \right)^{-1} \symbf{W}^\top
\end{equation*}
\subsection{Orthogonal Projectors onto other Spaces}
Given the orthogonal projector \(\symbf{P} = \proj_V\) onto a subspace \(V\), the
orthogonal projector \(\symbf{Q} = \proj_{V^\perp}\) onto \(V^\perp\) is given by
\begin{equation*}
    \symbf{Q} = \symbf{I} - \symbf{P}
\end{equation*}
This is because the vector \(\symbfit{b}\) can be represented as the
sum of the projections onto \(V\) and \(V^\perp\)
\begin{equation*}
    \symbfit{b} = \symbf{P}\symbfit{b} + \symbf{Q}\symbfit{b}.
\end{equation*}
The dot product of these two vectors is therefore also zero, given that
both projections lie in orthogonal subspaces.
\begin{equation*}
    \left( \symbf{P}\symbfit{b} \right)^\top \symbf{Q}\symbfit{b} = \symbf{0}
\end{equation*}
or
\begin{equation*}
    \symbf{P} \symbf{Q} = \symbf{0}
\end{equation*}
where \(\symbf{0}\) is the zero matrix.
\section{Orthogonal Matrices}
\subsection{Standard Basis Vectors}
The standard basis vectors are constructed by placing a \(1\)
in the \(n\)th row of the \(n\)th basis vector in \(\R^n\).
\begin{equation*}
    \symbfit{e}_1 =
    \begin{bmatrix}
        1      \\
        0      \\
        \vdots \\
        0
    \end{bmatrix}
    ,\: \dots,\: \symbfit{e}_n =
    \begin{bmatrix}
        0      \\
        0      \\
        \vdots \\
        1
    \end{bmatrix}
\end{equation*}
\subsection{Standard Basis}
In \(\R^n\), the standard basis \(S\) consists of the vectors
\begin{equation*}
    S = \left\{ \symbfit{e}_1,\: \dots,\: \symbfit{e}_n \right\}
\end{equation*}
where the coefficients of a column vector implicitly represent the coefficients
of the linear \linebreak combination of basis vectors in \(S\).

For example, the vector \(\symbfit{b}_S\) in the standard basis
\begin{equation*}
    \symbfit{b}_S = b_1 \symbfit{e}_1 + \cdots + b_n \symbfit{e}_n
\end{equation*}
can be represented as a column vector
\begin{equation*}
    \symbfit{b}_S = \begin{bmatrix}
        b_1    \\
        \vdots \\
        b_n
    \end{bmatrix}
\end{equation*}
where \(b_1,\: \dots,\: b_n\) are the entries of \(\symbfit{b}_S\).
\subsection{Change of Basis}
If we wanted to represent the vector \(\symbfit{b}_S\) in terms of another basis
\(W\), where
\begin{equation*}
    W = \left\{ \symbfit{w}_1,\: \dots,\: \symbfit{w}_n \right\}
\end{equation*}
we would do so by solving the following linear system
\begin{align*}
    c_1 \symbfit{w}_1 + \cdots + c_n \symbfit{w}_n & = \symbfit{b}_S \\
    \begin{bmatrix}
        \symbfit{w}_1 & \cdots & \symbfit{w}_n
    \end{bmatrix} \begin{bmatrix}
                      c_1    \\
                      \vdots \\
                      c_n
                  \end{bmatrix}        & = \symbfit{b}_S             \\
    \symbf{W} \symbfit{c}                          & = \symbfit{b}_S
\end{align*}
The solution to this system allows us to represent \(\symbfit{b}_S\) as
\(\symbfit{b}_W\), where the value of \(\symbfit{b}\) remains the same, but
it is represented using a different basis.
\begin{equation*}
    \symbfit{b}_W = \begin{bmatrix}
        c_1 \symbfit{w}_1 \\
        \vdots            \\
        c_n \symbfit{w}_n
    \end{bmatrix}
\end{equation*}
\subsection{Orthonormal Basis}
If we consider a basis \(Q\), such that every basis vector is normalised
and orthogonal to every other basis vector, then \(Q\) is an orthonormal basis.
\begin{definition}[Kronecker delta]
    We can summarise such a basis using the Kronecker delta \(\delta_{ij}\),
    which is defined
    \begin{equation*}
        \delta_{ij} = \begin{cases}
            1, & i = j   \\
            0, & i \ne j
        \end{cases}
    \end{equation*}
    so that in an orthonormal basis \(Q\) with basis vectors
    \(\left\{ \symbfit{q}_1,\: \dots,\: \symbfit{q}_n \right\}\)
    \begin{equation*}
        \symbfit{q}_i \cdot \symbfit{q}_j = \delta_{ij}
    \end{equation*}
\end{definition}
By using an orthonormal basis, we can determine the coefficients of \(\symbfit{b}_Q\) without
solve a linear system. For example, the \(i\)th coefficient can be determined as
follows
\begin{align*}
    c_1 \symbfit{q}_1 + \cdots + c_i \symbfit{q}_i + \cdots + c_n \symbfit{q}_n                                                                                                       & = \symbfit{b}_S                     \\
    \symbfit{q}_i \cdot \left( c_1 \symbfit{q}_1 + \cdots + c_i \symbfit{q}_i + \cdots + c_n \symbfit{q}_n \right)                                                                    & = \symbfit{q}_i \cdot \symbfit{b}_S \\
    c_1 \cancelto{0}{\symbfit{q}_i \cdot \symbfit{q}_1} + \cdots + c_i \cancelto{1}{\symbfit{q}_i \cdot \symbfit{q}_i} + \cdots + c_n \cancelto{0}{\symbfit{q}_i \cdot \symbfit{q}_n} & = \symbfit{q}_i \cdot \symbfit{b}_S \\
    c_i                                                                                                                                                                               & = \symbfit{q}_i \cdot \symbfit{b}_S
\end{align*}
Therefore
\begin{align*}
    \symbfit{b}_Q & = \begin{bmatrix}
                          \symbfit{q}_i \cdot \symbfit{b}_S \\
                          \vdots                            \\
                          \symbfit{q}_n \cdot \symbfit{b}_S
                      \end{bmatrix} \\
                  & = \begin{bmatrix}
                          \symbfit{q}_i^\top \symbfit{b}_S \\
                          \vdots                           \\
                          \symbfit{q}_n^\top \symbfit{b}_S
                      \end{bmatrix}  \\
                  & = \begin{bmatrix}
                          \symbfit{q}_i^\top \symbfit{b}_S \\
                          \vdots                           \\
                          \symbfit{q}_n^\top \symbfit{b}_S
                      \end{bmatrix}  \\
                  & = \symbf{Q}^\top \symbfit{b}_S
\end{align*}
\subsection{Projection onto a Vector}
Consider a subspace which consists of one vector \(\symbfit{a}\), the projection of \(\symbfit{b}\)
onto \(\symbfit{a}\) is given by
\begin{align*}
    \proj_{\symbfit{a}} \left( \symbfit{b} \right) = \symbfit{b}_P & = \symbfit{a} \left( \symbfit{a}^\top \symbfit{a} \right)^{-1} \symbfit{a}^\top \symbfit{b} \\
                                                                   & = \frac{\symbfit{a}}{\norm{\symbfit{a}}^2} \symbfit{a} \cdot \symbfit{b}
\end{align*}
If we instead project \(\symbfit{b}\) onto a unit vector \(\symbfit{q}\), then this simplifies to
\begin{equation*}
    \proj_{\symbfit{q}} \left( \symbfit{b} \right) = \symbfit{q} \left( \symbfit{q} \cdot \symbfit{b} \right)
\end{equation*}
\subsection{Gram-Schmidt Process}
To convert an arbitrary basis \(W\) to an orthonormal basis \(Q\), we
must develop a process that is generalisable to any basis.

We construct \(Q\) as follows
\begin{align*}
    \symbfit{v}_1 & = \symbfit{w}_1                                                                                                           & \symbfit{q}_1 & = \frac{\symbfit{v}_1}{\norm{\symbfit{v}_1}} \\
    \symbfit{v}_2 & = \symbfit{w}_2 - \proj_{\symbfit{q}_1} \left( \symbfit{w}_2 \right)                                                      & \symbfit{q}_2 & = \frac{\symbfit{v}_2}{\norm{\symbfit{v}_2}} \\
    \symbfit{v}_3 & = \symbfit{w}_3 - \proj_{\symbfit{q}_1} \left( \symbfit{w}_3 \right) - \proj_{\symbfit{q}_2} \left( \symbfit{w}_3 \right) & \symbfit{q}_3 & = \frac{\symbfit{v}_3}{\norm{\symbfit{v}_3}} \\
                  & \vdots                                                                                                                    &               & \vdots                                       \\
    \symbfit{v}_i & = \symbfit{w}_i - \sum_{j = 1}^{i - 1} \proj_{\symbfit{q}_j} \left( \symbfit{w}_i \right)                                 & \symbfit{q}_i & = \frac{\symbfit{v}_i}{\norm{\symbfit{v}_i}} 
\end{align*}
\end{document}
